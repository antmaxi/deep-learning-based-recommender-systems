{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    \"\"\"Construct dataset for deep learning project\"\"\"\n",
    "    def __init__(self, ratings):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        preprocess_ratings = self._binarize(ratings)\n",
    "        self.user_pool = set(self.ratings['userId'].unique())\n",
    "        self.item_pool = set(self.ratings['itemId'].unique())\n",
    "        # create negative item samples for Mem learning\n",
    "        self.negatives = self._sample_negative(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._train_test_split_loo(preprocess_ratings)\n",
    "\n",
    "    \n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
    "        return ratings\n",
    "\n",
    "    def _train_test_split_loo(self, ratings):\n",
    "        \"\"\"leave one out train/test split \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        # Test sample are the ones with the heighest time stamp\n",
    "        test = ratings[ratings['rank_latest'] == 1]\n",
    "        # all others are in the traingis set\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        # Each user should at least have rated x samples => both sets should contain the same userIds\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def _sample_negative(self, ratings):\n",
    "        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n",
    "        # Creates for each unique user a set of items that he interacted with\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
    "        return interact_status[['userId', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def save(self, filename, format=\"CMN\"):\n",
    "        \"\"\"\n",
    "        CMN format required\n",
    "        - train_data.npy\n",
    "            [[user id, item id], ...]\n",
    "        - test_data.npy\n",
    "            {userid: (pos_id, [neg_id1, neg_id2, ..., neg_id100])}\n",
    "        \"\"\"\n",
    "        if format == \"CMN\":\n",
    "            train_data = self.train_ratings[[\"userId\",\"itemId\"]].to_numpy()\n",
    "            test_data = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
    "            test_data = test_data[[\"userId\",\"itemId\",\"negative_samples\"]]\n",
    "            #test_data = test_data.apply(lambda r: {r[\"userId\"]: (r['itemId'], r[\"negative_samples\"])}, axis=1).to_numpy()\n",
    "            test_data = dict([(i,(a, b)) for i, a,b in zip(test_data.userId, test_data.itemId, test_data.negative_samples)])\n",
    "            np.savez(filename, train_data=train_data, test_data=test_data)\n",
    "            \n",
    "        elif format == \"NCF\":\n",
    "            pass\n",
    "    # def instance_a_train_loader(self, num_negatives, batch_size):\n",
    "    #     \"\"\"instance train loader for one training epoch\"\"\"\n",
    "    #     users, items, ratings = [], [], []\n",
    "    #     train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
    "    #     train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "    #     for row in train_ratings.itertuples():\n",
    "    #         users.append(int(row.userId))\n",
    "    #         items.append(int(row.itemId))\n",
    "    #         ratings.append(float(row.rating))\n",
    "    #         for i in range(num_negatives):\n",
    "    #             users.append(int(row.userId))\n",
    "    #             items.append(int(row.negatives[i]))\n",
    "    #             ratings.append(float(0))  # negative samples get 0 rating\n",
    "    #     dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
    "    #                                     item_tensor=torch.LongTensor(items),\n",
    "    #                                     target_tensor=torch.FloatTensor(ratings))\n",
    "    #     return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # @property\n",
    "    # def evaluate_data(self):\n",
    "    #     \"\"\"create evaluate data\"\"\"\n",
    "    #     test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
    "    #     test_users, test_items, negative_users, negative_items = [], [], [], []\n",
    "    #     for row in test_ratings.itertuples():\n",
    "    #         test_users.append(int(row.userId))\n",
    "    #         test_items.append(int(row.itemId))\n",
    "    #         for i in range(len(row.negative_samples)):\n",
    "    #             negative_users.append(int(row.userId))\n",
    "    #             negative_items.append(int(row.negative_samples[i]))\n",
    "    #     return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n",
    "    #             torch.LongTensor(negative_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ML1M-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n"
     ]
    }
   ],
   "source": [
    "ml1m_dir = '/home/pollakg/polybox/CSE/master/2nd_term/Deep Learning/project/project-git/data/ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
    "# Reindex\n",
    "unique_user_id = ml1m_rating[['uid']].drop_duplicates().reindex() # Create df of unique users\n",
    "unique_user_id['userId'] = np.arange(len(unique_user_id)) # append userId [uid, userId] [starts from 0, starts from 1]\n",
    "# Merge based on same uid => add userId with corresponding fitting uid\n",
    "ml1m_rating = pd.merge(ml1m_rating, unique_user_id, on=['uid'], how='left')\n",
    "unique_item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "unique_item_id['itemId'] = np.arange(len(unique_item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, unique_item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Epinions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pollakg/.local/share/virtualenvs/master-Ia_jUSae/lib/python3.7/site-packages/pandas/core/frame.py:4238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>itemId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>4.0</td>\n      <td>1027296000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1201305600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>1118016000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2</td>\n      <td>4.0</td>\n      <td>1149292800</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>5.0</td>\n      <td>1012262400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>188473</th>\n      <td>2351</td>\n      <td>41268</td>\n      <td>2.0</td>\n      <td>1012608000</td>\n    </tr>\n    <tr>\n      <th>188474</th>\n      <td>60</td>\n      <td>41268</td>\n      <td>4.0</td>\n      <td>1016409600</td>\n    </tr>\n    <tr>\n      <th>188475</th>\n      <td>116258</td>\n      <td>41268</td>\n      <td>1.0</td>\n      <td>1051747200</td>\n    </tr>\n    <tr>\n      <th>188476</th>\n      <td>4263</td>\n      <td>41268</td>\n      <td>5.0</td>\n      <td>1029974400</td>\n    </tr>\n    <tr>\n      <th>188477</th>\n      <td>116259</td>\n      <td>41268</td>\n      <td>5.0</td>\n      <td>1274227200</td>\n    </tr>\n  </tbody>\n</table>\n<p>188478 rows × 4 columns</p>\n</div>"
      ],
      "text/plain": [
       "        userId  itemId  rating   timestamp\n0            0       0     4.0  1027296000\n1            1       1     2.0  1201305600\n2            2       1     4.0  1118016000\n3            3       2     4.0  1149292800\n4            4       3     5.0  1012262400\n...        ...     ...     ...         ...\n188473    2351   41268     2.0  1012608000\n188474      60   41268     4.0  1016409600\n188475  116258   41268     1.0  1051747200\n188476    4263   41268     5.0  1029974400\n188477  116259   41268     5.0  1274227200\n\n[188478 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse file\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# read file\n",
    "lines = []\n",
    "with open('../data/epinions/epinions_data/epinions.json', 'r') as f:\n",
    "    for nb, line in enumerate(f): \n",
    "        lines.append(ast.literal_eval(line))\n",
    "        \n",
    "df = pd.DataFrame(lines)\n",
    "epi_rating = df[[\"user\", \"item\", \"stars\", \"time\"]]\n",
    "epi_rating.rename(columns={'stars':'rating', 'time':'timestamp'}, inplace=True)\n",
    "# Reindex\n",
    "unique_user_id = epi_rating[['user']].drop_duplicates().reindex() # Create df of unique users\n",
    "unique_user_id['userId'] = np.arange(len(unique_user_id)) # append userId [uid, userId] [starts from 0, starts from 1]\n",
    "# Merge based on same uid => add userId with corresponding fitting uid\n",
    "epi_rating = pd.merge(epi_rating, unique_user_id, on=['user'], how='left')\n",
    "unique_item_id = epi_rating[['item']].drop_duplicates()\n",
    "unique_item_id['itemId'] = np.arange(len(unique_item_id))\n",
    "epi_rating = pd.merge(epi_rating, unique_item_id, on=['item'], how='left')\n",
    "epi_rating = epi_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "epi_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Generating statiscts/features for each data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Epinsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = DataGenerator(epi_rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Move Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ratings', 'user_pool', 'item_pool', 'negatives', 'train_ratings', 'test_ratings'])\n"
     ]
    }
   ],
   "source": [
    "# Movielense\n",
    "df = DataGenerator(ml1m_rating)\n",
    "print(df.__dict__.keys())\n",
    "df.save(\"/home/pollakg/polybox/CSE/master/2nd_term/Deep Learning/project/project-git/data/ml-1m/ml.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Just for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ml1m_dir = '/home/pollakg/polybox/CSE/master/2nd_term/Deep Learning/project/project-git/data/ml-1m/ml.npz'\n",
    "_data = np.load(ml1m_dir, allow_pickle=True)\n",
    "train_data = _data['train_data'][:, :2]\n",
    "test_data_ = _data['test_data']\n",
    "test_data_.tolist().keys()\n",
    "\n",
    "# # Neighborhoods\n",
    "# user_items = defaultdict(set)\n",
    "# item_users = defaultdict(set)\n",
    "# for u, i in train_data:\n",
    "#     user_items[u].add(i)\n",
    "#     item_users[i].add(u)\n",
    "# # Get a list version so we do not need to perform type casting\n",
    "# item_users_list = {k: list(v) for k, v in item_users.items()}\n",
    "# # maximum number of users that rated an item i\n",
    "# _max_user_neighbors = max([len(x) for x in self.item_users.values()])\n",
    "# user_items = dict(self.user_items)\n",
    "# item_users = dict(self.item_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "files = [\"citeulike-a.npz\", \"pinterest.npz\"]\n",
    "npz_files = []\n",
    "test = {}\n",
    "train = {}\n",
    "for f in files:\n",
    "    npz = np.load(f, allow_pickle=True)\n",
    "    train[f] = npz[\"train_data\"]\n",
    "    test[f] = npz[\"test_data\"]\n",
    "    #print(len(test[f]), len(test[f][0]),\"[1, \",len(test[f][0][1]),\"]\")\n",
    "\n",
    "train[\"pinterest.npz\"][:, :2]\n",
    "test[\"pinterest.npz\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1445622, 2)\n",
      "()\n",
      "[[ 0  3]\n",
      " [ 0  4]\n",
      " [ 0  5]\n",
      " [ 0  6]\n",
      " [ 0  7]\n",
      " [ 0  8]\n",
      " [ 0  9]\n",
      " [ 0 10]\n",
      " [ 0 11]\n",
      " [ 0  0]\n",
      " [ 0 13]\n",
      " [ 0 14]\n",
      " [ 0 15]\n",
      " [ 0 16]\n",
      " [ 0 17]\n",
      " [ 0 18]\n",
      " [ 0 19]\n",
      " [ 0 20]\n",
      " [ 0  0]]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(type(train[\"pinterest.npz\"]))\n",
    "print(type(test[\"pinterest.npz\"]))\n",
    "print(train[\"pinterest.npz\"].shape)\n",
    "print(test[\"pinterest.npz\"].shape)\n",
    "print(train[\"pinterest.npz\"][1:20])\n",
    "print(len(test[\"pinterest.npz\"].tolist()[0][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/pollakg/.local/share/virtualenvs/master-Ia_jUSae/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "master-kernel",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "master-kernel"
  },
  "name": "data_exploration.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
