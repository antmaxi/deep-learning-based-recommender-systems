%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:

%%% NCF

%%% CMN

%%% NGCF
The NGCF approach shows itself among other assessed methods as the best on the Jester dataset 
and as very good on the Movielens, but as the worst on the Epinions dataset.
It could be explained with differences in density of these datasets, because
Epinions is very sparse and, probably, it's hard for NGCF to incorporate 
collaborative signal into embeddings in these circumstances.
Also, training takes from about an hour (Jester and Epinions) to 6 hours (Movielens),
what is, in general, worse than other methods.

%%% VAE
The VAE performed pretty well compared to the other methods on all tested datasets. 
Only on Epinions it falls a bit short. 
One of it's main strength is certainly that even on a i7-3615QM quad-core CPU with 8GB of RAM it could be trained and evaluated in minutes on Movielens and Jester, and in about an hour on Epinions.

%%% Ensemble
Combining different CF methods experimentally seems to work since the ensemble approach managed to achieve the best scores on the Movielens as well as Jester datasets. 
A little experimenting on the subsets of the methods showed that the increased scores mainly  come from combining the VAE with one of the other methods. 
This suggests that the VAE indeed finds different interesting relationships in the data than the other methods. 
However, combining methods at such a high level like we did also bears problems, mainly the performance and flexibility will be dictated by the slowest and least flexible method in the ensemble. 
The decrease (compared to the individual methods) on the Epinions dataset might be related to the fact that our individual methods already perform pretty poorly. 
Maybe a certain performance threshold on the individual methods is necessary for the ensemble method to have increased performance.

%%%% 
The fact that the method took 24h in order to compute three epochs (GeForce GTX 1080 Ti) for the jester data set can most likely be contributed to the density of the data set as well as the computational complexity of the method $\bigO\left(d\abs{N}(\idxi)+d^2+d\right)\propto\abs{N(\idxi)}$.\\
Also is seems to be logic that the attention mechanism needs a lot of training interactions in order to perform well, which can be seen by the improved accuracy of the movielens data set.
One thing that has to be mentioned is that we could not reproduce the same results as the original paper neither on our epinions data split nor on the original data (as the epinions data was missing).