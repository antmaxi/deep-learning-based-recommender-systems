\textbf{Datasets.}
We study the effectiveness of the aforementioned neural recommendation approaches on three publicly available datasets, i.e. Movielens \cite{harper2016movielens}, Jester \cite{jester}, and Epinions \cite{epinions}. 
The main characteristics of these datasets are summarized in Table \ref{tab:datasets}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
        \hline
        Dataset & \#Users & \#Items & \#Interactions & Density \\
        \hline
        Movielens &  6,040 &  3,706 & 1,000,209 & 0.0447 \\
        Jester    & 24,938 &    100 &   616,912 & 0.2474 \\
        Epinions  & 27,453 & 37,274 &    99,321 & 0.0001
    \end{tabular}
    \caption{Dataset statistics.}
    \label{tab:datasets}
\end{table}

Movielens is a movie rating dataset that has been widely utilized as a benchmark for evaluating collaborative filtering algorithms.
In our work, we use the version containing nearly one million ratings, where each user has rated at least 20 movies.
Jester, on the other hand, is a joke rating dataset with a lot more users, but a lot fewer items compared to Movielens.
We use the version where each user has rated between 15 and 35 jokes.
Epinions is a dataset containing consumer reviews for various products. 
This is a very sparse dataset, i.e. most of the users have rated very few items, a fact that leads to the existence of a very weak collaborative signal in the dataset. 
Therefore, Epinions is a very difficult benchmark for the selected methods, since all of them utilize the collaborative filtering effect, and thus, low quality recommendations are expected.

It should be stated, that although all the aforementioned datasets, include explicit feedback from users, we transformed them into implicit feedback datasets, in order to study the learning from the implicit signal.
To this end, we binarized the ratings, i.e. whenever there is a rating of a user to an item, either positive or negative, we set it to 1, since it denotes the existence of a user-item interaction. 
If there is no such interaction we set it to 0.

\textbf{Evaluation.}
We evaluate the quality of item recommendation using the leave-one-out evaluation method, following the prior work \cite{he2017neural,ebesu2018collaborative}.
In order to make the split as realistic as possible, for each user we held-out their latest interaction as the testset, and utilized the remaining data for training.
Then, we ranked the ``positive'' test item (i.e. item with the latest interaction by the user) against m randomly sampled ``negative'' items (i.e. items that this user has never interacted with).
We evaluated the ranking quality using the Hit Ratio (HR@k), and the Normalized Discounted Cumulative Gain (NDCG@k) metrics.
Intuitively, HR@k measures the presence of the ``positive'' item within the top k items, while NDCG@k measures the items' position in the ranked list, penalizing the score for ranking the item lower in that list.
We computed both metrics for each test user and for k=10, and reported the average score.

It should be stated, that since the Jester dataset does not include timestamps, we generated a random timestamp for each rating, and then proceeded to the train-test split. 
Moreover, it should be mentioned that for the case of Epinions, we filtered out from the dataset all the users that have only rated a single item, so as to avoid the cold-start setting (for the users).
Finally, for Movielens and Epinions we set m=99, while for Jester we set m=49, since there are only 100 items in the dataset in total, while there are users that have rated up to 35 items.
